# Modul-MSVC

## Minimal Flask Server

https://gitlab.com/ch-tbz-wb/Stud/MSVC/-/blob/main/2_Unterrichtsressourcen/B_Microservices_lokal/01_minimal_dockerized_flask.md?ref_type=heads

Hier ist die Zusammenfassung im **Markdown-Format**:  


# Python und API-Entwicklung

Python ist eine vielseitige Programmiersprache, die sich besonders f√ºr die API-Entwicklung eignet. Beliebte Frameworks daf√ºr sind **Django, Flask und FastAPI**:

- **Django**: Vollumf√§ngliches Framework mit vielen integrierten Features.
- **Flask**: Minimalistisches Framework, das flexibel durch externe Bibliotheken erweitert werden kann.
- **FastAPI**: Moderne L√∂sung mit **Asynchronit√§t** und **Typhinweisen**, besonders schnell.

Der Autor bevorzugt **Flask und FastAPI** und gibt Tipps zur API-Entwicklung mit Flask.

---

## Best Practices f√ºr API-Entwicklung

### 1. Klare und konsistente Endpunkt-Namen und HTTP-Verben

- **Ressourcen sollten als Plural-Nomen benannt werden** (z. B. `/users`, `/users/{userId}`).
- **Trennzeichen**:
  - **Bindestriche (`-`)** f√ºr bessere Lesbarkeit (`/users/{userId}/mobile-devices`).
  - **Schr√§gstriche (`/`)** zur Darstellung der Hierarchie.
  - **Kleinbuchstaben** f√ºr URLs.
- **CRUD-Operationen mit HTTP-Verben**:
  - `GET /users` ‚Üí Liste aller Nutzer
  - `POST /users` ‚Üí Neuen Nutzer erstellen
  - `PUT /users/{userId}` ‚Üí Nutzer aktualisieren
  - `DELETE /users/{userId}` ‚Üí Nutzer l√∂schen
  - `PATCH /users/{userId}` ‚Üí Teilweise Aktualisierung

üö´ **Falsch**:
`/users/get-all`, `/users/create`, `/users/{userId}/list-orders`.

---

### 2. Strukturierung der Anwendung

Es gibt keine feste Regel f√ºr den Aufbau einer Flask-Anwendung, aber eine bew√§hrte Struktur ist:

```plaintext
project/
‚îÇ‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ route/
‚îÇ   ‚îú‚îÄ‚îÄ schema/
‚îÇ   ‚îú‚îÄ‚îÄ service/
‚îÇ‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ route/
‚îÇ‚îÄ‚îÄ app.py
‚îÇ‚îÄ‚îÄ Pipfile
‚îÇ‚îÄ‚îÄ .gitignore
```

- **Models**: Datenbeschreibungen (z. B. Datenbank-Modelle).
- **Routes**: Definiert API-Endpunkte.
- **Schemas**: Validierung der Eingaben/Ausgaben.
- **Services**: Gesch√§ftslogik, um den Code in Routen schlank zu halten.

Blueprints helfen dabei, Routen zu organisieren:

```python
home_api = Blueprint('api', __name__, url_prefix='/home-service')
```

---

### 3. Automatische API-Dokumentation mit Swagger

Anstatt Dokumentation manuell zu schreiben, kann **Swagger** genutzt werden:

- **Flasgger** ist eine Bibliothek f√ºr Flask, die automatisch interaktive API-Dokumentation erstellt.
- **Swagger-Daten** werden aus Code-Kommentaren und `@swag_from` Annotationen bezogen.

```python
@swag_from({
    'responses': {
        HTTPStatus.OK.value: {
            'description': 'Welcome message',
            'schema': WelcomeSchema
        }
    }
})
```

---

### 4. Test-Driven Development (TDD)

Tests sind wichtig f√ºr stabile APIs. **TDD (Test-Driven Development)** bedeutet:

1. **Test zuerst schreiben**:

    ```python
    def test_answer():
        assert sum_two_numbers(3, 5) == 8
    ```

2. **Code schreiben, damit der Test besteht**:

    ```python
    def sum_two_numbers(num1, num2):
        return num1 + num2
    ```

3. **Test ausf√ºhren** ‚Äì Falls er fehlschl√§gt, Code korrigieren.

---

## Fazit

Es gibt nicht **die eine** richtige Methode f√ºr API-Entwicklung, aber **Konsistenz, Modularit√§t, Dokumentation und Tests** sind essenziell.  
Durch die Befolgung dieser Best Practices kann man **effizienter APIs entwickeln und deren Qualit√§t steigern**. üöÄ

Dieses Markdown-Format kann direkt in einem Markdown-Editor oder GitHub verwendet werden.

# CRUD

Die CRUD-Operationen (Create, Read, Update, Delete) sind grundlegende Funktionen beim Aufbau von APIs, die es erm√∂glichen, Daten zu erstellen, abzurufen, zu aktualisieren und zu l√∂schen. Diese vier Operationen sind oft notwendig, um ein vollst√§ndiges und benutzbares Modell zu schaffen. Wenn eine Aktion nicht durch eine dieser vier Operationen beschrieben werden kann, sollte sie m√∂glicherweise als eigenes Modell betrachtet werden.

Im Zusammenhang mit REST-APIs entsprechen CRUD-Operationen oft den HTTP-Methoden POST (Create), GET (Read), PUT (Update) und DELETE (Delete). Dies sind fundamentale Elemente f√ºr ein persistenten Speichersystem.

Beispiel eines RESTful Systems f√ºr ein Restaurantmen√º:
- Create: Um ein neues Gericht hinzuzuf√ºgen, wird eine POST-Anforderung verwendet. Der Server antwortet mit einem HTTP-Code 201 (Created).
- Read: Mit GET wird die gesamte Liste von Gerichten oder ein spezifisches Gericht abgerufen, ohne dass Daten ver√§ndert werden. Erfolgreiche Anfragen erhalten den Statuscode 200 (OK).
- Update: PUT wird verwendet, um ein Gericht zu aktualisieren, wie z. B. eine Preis√§nderung. Der Server antwortet mit 200 (OK).
- Delete: Mit DELETE wird ein Gericht aus dem Men√º entfernt. Der Server antwortet mit 204 (No Content), ohne eine Antwort im Body zu liefern.

CRUD-Operationen bieten eine einfache und klare Struktur f√ºr die Entwicklung und Verwaltung von APIs und Datenbanken.

CRUD ist eine Abk√ºrzung aus dem Datenmanagement und steht f√ºr die vier grundlegenden Operationen in Datenbanken: **Create** (Erstellen), **Read** (Lesen), **Update** (Aktualisieren) und **Delete** (L√∂schen). Diese Funktionen erm√∂glichen die Verwaltung von Daten in persistenten Datenbanken.

Einsatzzweck:
- Datenbank-Experten nutzen CRUD-Operationen, um Datenbankprobleme zu beheben oder Daten zu bereinigen.
- Endanwender verwenden CRUD, um beispielsweise Konten zu erstellen, zu √§ndern oder zu l√∂schen.

CRUD dient auch als eine Checkliste f√ºr Entwickler, um sicherzustellen, dass ein Anwendungsmodell alle vier Operationen unterst√ºtzt. Wenn eine Aktion nicht durch CRUD beschrieben werden kann, sollte ein eigenes Modell entwickelt werden.

Sprachumgebungen: CRUD wird in verschiedenen Programmiersprachen und Plattformen verwendet, z. B. Java, JavaScript, PHP, .NET und Python. In SQL entsprechen die CRUD-Operationen den Befehlen `INSERT`, `SELECT`, `UPDATE` und `DELETE`. Bei RESTful HTTP sind es `POST`, `GET`, `PUT` und `DELETE`.

CRUD-Frameworks: Diese Frameworks erm√∂glichen es, Datenbankobjekte √ºber eine grafische Oberfl√§che darzustellen und zu bearbeiten. Sie sind besonders in Mehrbenutzersystemen n√ºtzlich, da mehrere Benutzer gleichzeitig Daten einsehen k√∂nnen. 

Anwendungsbeispiel: Eine einfache Aufgabenlisten-App verwendet CRUD, um Aufgaben hinzuzuf√ºgen, anzuzeigen, zu √§ndern und zu l√∂schen.

Nachteile und Grenzen: CRUD ist oft zu einfach, um komplexere Anforderungen abzubilden. Beispielsweise werden gel√∂schte oder aktualisierte Daten nicht automatisch historisiert, was in einigen Anwendungen problematisch sein kann. Diese Einschr√§nkungen k√∂nnen durch zus√§tzliche Felder oder Tabellen zur Speicherung von √Ñnderungen behoben werden.

# Migrate Funktion

![Migrate](image.png)

# Testing

Test suite starten:
`docker compose -f compose.test.yaml up --build`

# Image verkleinern

## Noch kleineres Docker-Image mit Multi-Stage Build:
```dockerfile
# STAGE 1: Build-Umgebung
FROM python:3.10.10-alpine AS builder

WORKDIR /app

# Installiere nur PIP, um das Image klein zu halten
RUN apk add --no-cache gcc musl-dev libffi-dev

# Kopiere nur die Requirements und installiere Abh√§ngigkeiten
COPY requirements.txt .
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt && \
    pip install --no-cache-dir --prefix=/install gunicorn

# STAGE 2: Minimales Laufzeit-Image
FROM python:3.10.10-alpine

WORKDIR /app

# Kopiere nur die ben√∂tigten Dateien aus der Build-Stage
COPY --from=builder /install /usr/local
COPY . .

# Exponiere den Port f√ºr Flask
EXPOSE 5000

# Starte den Gunicorn-Server
CMD ["gunicorn", "-b", "0.0.0.0:5000", "wsgi:app"]
```

---

## Optimierungen und Einsparungen:
1. **Multi-Stage Build:**  
   - Die Abh√§ngigkeiten werden in einer separaten "Builder"-Stage installiert.  
   - Dadurch bleiben keine Compiler- und Build-Tools im finalen Image √ºbrig.  

2. **Kleinste Basis (`alpine`)**:  
   - `python:3.10.10-alpine` ist viel schlanker als `slim-buster`.  

3. **Nur ben√∂tigte Systempakete (`apk add`)**:  
   - `gcc`, `musl-dev`, `libffi-dev` sind nur in der **Build-Stage** n√∂tig, verschwinden aber im finalen Image.  

4. **`--no-cache-dir` f√ºr PIP**:  
   - Spart weiteren Speicher, da PIP keinen Cache speichert.  

5. **Kein `ADD . /app` mehr**:  
   - Stattdessen **`COPY . .`**, um unn√∂tige Dateien zu vermeiden.  

### **Erwartete Einsparung**:
- Das Original-Image (`python:3.10.10-slim-buster`) hat **~35-40 MB**.  
- Mit `alpine` und Multi-Stage k√∂nnte das finale Image nur **10-15 MB** gro√ü sein.  

Falls du noch mehr Platz sparen willst, kannst du in `requirements.txt` pr√ºfen, ob unn√∂tige Abh√§ngigkeiten entfernt werden k√∂nnen.


# EC2 Instanz

## Estellen:

Betriebssystem: Amazon Linux als OS (Amazon Linux 2023 AMI)
Instance Typ: t2.micro (ist ausreichend, eine t2.medium sollte auch f√ºr die Kursdauer gehen ($50))
Schl√ºsselpaar: neues Schl√ºsselpaar erstellen
guten Namen einfallen lassen (z.B.: msvc-key)
ED25519 und .pem (SSH) ausw√§hlen. unbedingt den generierten .pem-File speichern. Wir brauchen ihn unbedingt f√ºr die Verbindung zu unserem Server
Netzwerkeinstellungen: SSH, HTTP, HTTPS freischalten.
alles andere Standardeinstellungen
unter "erweiterte Details" - ganz unten bei "Benutzerdaten/User data" folgeendes Script einf√ºgen:

```sh
#! /bin/sh
yum update -y
yum install git -y
yum install docker -y
service docker start
usermod -a -G docker ec2-user
chkconfig docker on
# install docker compose
wget https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)
mv docker-compose-$(uname -s)-$(uname -m) /usr/libexec/docker/cli-plugins/docker-compose
chmod +x /usr/libexec/docker/cli-plugins/docker-compose
systemctl enable docker.service --now
usermod -a -G docker ec2-user
```
## IP erstellen

1. EC2-Dashboard -> Elastic IP Adressen -> Elastic IP Adresse zuweisen
2. Alle Einstellungen auf Standard -> Zuweisen klicken.
3. In der √úbersicht auf die neue IP-Adresse klicken -> "Elastic-IP-Adresse zuordnen" klicken
4. bei Instance unsere gerade erstellte EC2 Instanz ausw√§hlen.
5. auf "Zuordnen" klicken
6. Die IP Adresse (z.B.: 34.198.175.193) irgendwo notieren (z.B.: OneNote) - wir werden sie noch √∂fter brauchen.

## SSH erstellen

1. kopiere den .pem File (Private Key z.b.: msvc-key.pem) in das .ssh-Verzeichnis im Home-Verzeichnis (.ssh im user ordner, lokal)

2. Mit SSH Verbinden
   - Pfad zum private key als -i Parameter
   - user ist ec2-user
   - Elastic IP Adresse von vorhin verwenden

`ssh -i "~/.ssh/lab-key.pem" ec2-user@34.198.175.193`

   - kurz mit yes best√§tigen, dass der neue Host zur Liste der vertrauensw√ºrdigen Server hinzugef√ºgt wird.

## Git Clone

1. Git konfigurieren und neuen Public Key erstellen:

    - ssh-keygen -t ed25519 -C [ihre@email.ch]
    - git config --global user.email [ihre@email.ch]
    - git config --global user.name [ihr gitlab_username]
    - git config --global --list

2. Auf der Ec2 Instanz den Key mit cat anzeigen lassen und rauskopieren
   
3. Im Github den neuen Public Key registrieren (In den Settings hinzuf√ºgen)

4. In der Instanz das Git clonen `git clone git@gitlab.com:tbz-itcne23-msvc/blueprint-flask-prod.git` (pfad ist im Github im Repo unter code -> SSH zu finden)
   
5. starten mit `docker compose -f compose.prod.yaml up --build` (wichtig in den blueprint ordner wechseln)

6. Test im Browser mit IP Adresse


# CI/CD-Entwicklungsrichtlinien  

CI/CD-Pipelines sind ein zentraler Bestandteil der Entwicklungs- und Bereitstellungsprozesse in GitLab. Sie automatisieren Aufgaben wie das Erstellen, Testen und Bereitstellen von Code√§nderungen. Bei der Entwicklung von Funktionen, die mit Pipelines interagieren oder diese ausl√∂sen, m√ºssen Sicherheits- und Betriebsaspekte ber√ºcksichtigt werden.  

## Allgemeine Richtlinien  

1. **Pipelines als Schreiboperationen betrachten**  
   Das Ausl√∂sen einer Pipeline ver√§ndert den Systemzustand, z. B. durch Deployments oder Konfigurations√§nderungen. Diese Operationen sollten mit derselben Vorsicht behandelt werden wie andere kritische Schreibvorg√§nge.  

2. **Explizite Pipeline-Ausf√ºhrung**  
   Nutzer sollten sich bewusst sein, wenn eine Pipeline gestartet wird. Aktionen, die eine Pipeline ausl√∂sen, m√ºssen transparent gestaltet sein.  

3. **Isolierung und Sicherheit**  
   Pipeline-Jobs laufen in einer Remote-Umgebung. Es muss sichergestellt werden, dass sie keine sensiblen Daten oder Systeme unbeabsichtigt preisgeben.  

4. **Zusammenarbeit mit Sicherheitsteams**  
   Die Application Security (AppSec)- und Verify-Teams sollten fr√ºhzeitig einbezogen werden, um Sicherheitsrisiken zu identifizieren und zu minimieren.  

5. **Bestimmung des Pipeline-Akteurs**  
   Es sollte klar sein, welcher Benutzer eine Pipeline startet. Unsichere Szenarien, in denen der Ersteller einer Pipeline nicht mit dem Autor der Code√§nderungen √ºbereinstimmt, sollten vermieden werden.  

6. **Variabilit√§t der Job-Ausf√ºhrungsnutzer**  
   Der Nutzer, der einen Job ausf√ºhrt, kann sich √§ndern, z. B. bei manuellen Jobs oder Wiederholungen. Dies kann Auswirkungen auf Berechtigungen haben und muss ber√ºcksichtigt werden.  

7. **Einschr√§nkung des Operationsbereichs**  
   Neue CI/CD-Endpunkte sollten m√∂glichst auf einzelne Jobs oder Pipelines begrenzt werden, um Sicherheitsrisiken zu minimieren.  

8. **√úberwachung und Auditing**  
   Alle pipeline-relevanten Aktionen sollten protokolliert werden, einschlie√ülich Nutzerinformationen und Ereignisdetails.  

## Architektur√ºberblick  

Eine Pipeline kann durch verschiedene Ereignisse ausgel√∂st werden, z. B.:  
- Git-Push  
- API-Aufruf  
- Manuelles Starten durch den Nutzer  
- Merge-Request-√Ñnderungen  
- Geplante Pipelines  
- Upstream-Projekt-Abonnements  

Die **CreatePipelineService** verarbeitet diese Ereignisse und erstellt eine Pipeline basierend auf einer YAML-Konfiguration. Die **ProcessPipelineService** verwaltet anschlie√üend den Ablauf der Jobs bis zur Fertigstellung oder einem Fehler.  

Ein **Runner** f√ºhrt die Jobs aus, kommuniziert mit GitLab √ºber die Runner-API und meldet Status-Updates zur√ºck.  

## Job-Scheduling und Fehlerhandling  

- Jobs durchlaufen mehrere Statusphasen: *erstellt ‚Üí ausstehend ‚Üí laufend ‚Üí abgeschlossen/fehlgeschlagen*.  
- Runner w√§hlen Jobs basierend auf bestimmten Regeln aus, z. B. Projekt-, Gruppen- oder Instanzebene.  
- Jobs k√∂nnen aus der Warteschlange entfernt werden, wenn kein Runner verf√ºgbar ist oder wenn das Projekt sein CI/CD-Minuten-Kontingent √ºberschritten hat.  

## Definition von "Job" in GitLab CI/CD  

- **Ci::Build** ‚Äì Standard-Job f√ºr Runner  
- **Ci::Bridge** ‚Äì Erstellt eine untergeordnete Pipeline  
- **GenericCommitStatus** ‚Äì Externer CI/CD-Job (z. B. f√ºr Jenkins)  

Die Begriffe "Job" und "Build" sollten konsistent verwendet werden, um Missverst√§ndnisse zu vermeiden.  

## Fazit  

Diese Richtlinien helfen Entwicklern, sichere und effiziente CI/CD-Integrationen zu erstellen. Eine klare Definition von Benutzerrechten, Sicherheitsma√ünahmen und Pipeline-Abl√§ufen ist entscheidend f√ºr die Integrit√§t des Systems.

# Authentifizierungsdienste in verschiedenen Architekturen

## **Was ist ein Authentifizierungsdienst?**  
Authentifizierung dient dazu, die Identit√§t eines Nutzers zu √ºberpr√ºfen, um ihm Zugang und Berechtigungen im System zu gew√§hren. In monolithischen Anwendungen erfolgt dies innerhalb der Anwendung selbst. In einer Microservice-Architektur muss Authentifizierung jedoch anders umgesetzt werden, da das System aus vielen separaten Diensten besteht.

## **Authentifizierung in verschiedenen Architekturen**

## **1. Monolithische Architektur**  
In klassischen monolithischen Anwendungen wird die gesamte Authentifizierung innerhalb der Anwendung durchgef√ºhrt. Nach erfolgreicher Anmeldung wird eine Sitzung (Session) erstellt und auf dem Server gespeichert. Diese Session wird f√ºr alle weiteren Anfragen genutzt.

## **2. ESB-Architektur (Enterprise Service Bus)**  
ESB dient als Vermittler zwischen verschiedenen Diensten und erm√∂glicht deren Kommunikation. Da ESB eine Erweiterung der monolithischen Architektur ist, bleibt das Authentifizierungsverfahren im Wesentlichen unver√§ndert.

## **3. Microservice-Architektur**  
In einer Microservice-Architektur gibt es mehrere Herausforderungen f√ºr die Authentifizierung, da Benutzerinformationen nicht zentral gespeichert werden k√∂nnen. Hier gibt es drei Hauptans√§tze:

## **Authentifizierungsmethoden in Microservices**

## **1. Authentifizierung in jedem einzelnen Microservice**  
Jeder Microservice f√ºhrt seine eigene Authentifizierung durch.  

‚úÖ **Vorteile:**  
- Schnell zu implementieren  
- Jeder Service bleibt unabh√§ngig  

‚ùå **Nachteile:**  
- Hoher Wartungsaufwand durch duplizierten Code  
- Sicherheitslogik muss mehrfach implementiert werden  
- Schwierige √úberwachung und Verwaltung  

**Alternative Verbesserung:** Verwendung einer gemeinsamen Authentifizierungsbibliothek, die in jedem Microservice geladen wird. Dies reduziert doppelten Code, l√∂st aber nicht alle Probleme.

## **2. Zentrale Authentifizierung √ºber einen separaten Authentifizierungsdienst**  
Alle Authentifizierungsanfragen werden von einem zentralen Dienst verarbeitet.  

‚úÖ **Vorteile:**  
- Trennung der Zust√§ndigkeiten  
- Einheitliche Authentifizierung f√ºr alle Dienste  

‚ùå **Nachteile:**  
- Single Point of Failure (f√§llt der Dienst aus, gibt es keine Authentifizierung mehr)  
- Erh√∂hte Latenz, da jeder Microservice eine separate Anfrage stellen muss  

## **3. Authentifizierung √ºber ein API Gateway**  
Ein API Gateway dient als zentrale Schnittstelle f√ºr alle Anfragen und √ºbernimmt auch die Authentifizierung.  

‚úÖ **Vorteile:**  
- Schutz der Microservices vor direkten Angriffen  
- Einheitliche Authentifizierung f√ºr alle Dienste  
- Reduzierte Latenz  

‚ùå **Nachteile:**  
- Single Point of Failure (wenn das Gateway kompromittiert wird, sind alle Microservices betroffen)  

## **Fazit**

Je nach Anforderungen gibt es unterschiedliche Authentifizierungsl√∂sungen:  
‚úÖ **Monolithische Anwendungen:** Session-basierte Authentifizierung innerhalb der Anwendung.  
‚úÖ **Microservices:** Drei Alternativen:
1. **Jeder Microservice authentifiziert selbst** ‚Üí Schnell, aber wartungsintensiv.  
2. **Zentrale Authentifizierung √ºber einen separaten Dienst** ‚Üí Einheitlich, aber potenziell langsamer.  
3. **Authentifizierung √ºber ein API Gateway** ‚Üí Flexibel und effizient, aber sicherheitskritisch.  

Die beste L√∂sung h√§ngt von den spezifischen Anforderungen eines Systems ab. üöÄ